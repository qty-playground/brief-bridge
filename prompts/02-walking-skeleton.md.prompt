# Phase 2: Create Testing Walking Skeleton

## Objective
Build pytest-bdd infrastructure without real implementation - create the testing framework shell with NotImplementedError stubs.

**Critical Goal: Ensure all scenarios can be discovered by pytest-bdd without StepDefinitionNotFoundError**

## Prerequisites
- Feature file exists in `tests/{feature}/story.feature`
- All scenarios marked with @skip

## Required Reading
**MUST read these files before proceeding:**

Use the Read tool to load:
1. `tests/{specified_feature}/story.feature` - The feature file to implement
2. `docs/user-stories/{specified_feature}.md` - For step definitions reference
3. Current `pytest.ini` - To understand test configuration
4. `prompts/reference/python-coding-style.md.prompt` - For coding standards

## Step-by-Step Process

### Step 1: Analyze Feature File Steps
After reading the feature file, identify all unique step patterns:
- [ ] All Given steps (e.g., `Given client "{client_id}" exists with...`)
- [ ] All When steps (e.g., `When I POST to "{endpoint}" with:`)
- [ ] All Then steps (e.g., `Then response status code should be {code}`)

### Step 2: Create Test Infrastructure

Create this directory structure for `tests/{feature}/`:
```
tests/{feature}/
├── __init__.py                              # Empty file
├── story.feature                            # Already exists
├── test_{feature}.py                       # Step wrapper file (delegates to step modules)
├── given_create_client_with_status.py      # One step per module
├── given_setup_test_environment.py
├── when_submit_api_request.py              # One step per module
├── when_execute_command.py
├── then_verify_response_status.py          # One step per module
└── then_validate_response_body.py
```

### Step 3: Implement Step Wrapper File

**File:** `tests/{feature}/test_{feature}.py` (Only contains step wrappers)
```python
from pytest_bdd import scenarios, given, when, then, parsers
from tests.{feature} import given_create_client_with_status
from tests.{feature} import when_submit_api_request  
from tests.{feature} import then_verify_response_status

# Load all scenarios from feature file
scenarios('story.feature')

# Step wrappers - delegate to individual step modules
@given(parsers.parse('client "{client_id}" exists with status "{status}"'))
def given_client_with_status_exists(context, client_id, status):
    """Delegate to given_create_client_with_status step module"""
    context.client_id = client_id
    context.status = status
    given_create_client_with_status.invoke(context)

@when(parsers.parse('I POST to "{endpoint}" with:'))
def when_submit_request_to_api_endpoint(context, endpoint, docstring):
    """Delegate to when_submit_api_request step module"""
    context.endpoint = endpoint
    context.request_body = docstring
    when_submit_api_request.invoke(context)

@then(parsers.parse('response status code should be {status_code:d}'))
def then_api_response_has_expected_status(context, status_code):
    """Delegate to then_verify_response_status step module"""
    context.expected_status_code = status_code
    then_verify_response_status.invoke(context)
```

**Note:** The `context` fixture is now provided by root `conftest.py` with `ScenarioContext` class.

### Step 4: Implement Individual Step Modules

Each step module contains exactly ONE `invoke` function following Command Pattern:

**File:** `tests/{feature}/given_create_client_with_status.py`
```python
"""Given client exists with status - Screaming Architecture naming"""
from conftest import ScenarioContext
from brief_bridge.domain.entities.client import Client
from brief_bridge.domain.value_objects.client_id import ClientId
from brief_bridge.domain.value_objects.client_status import ClientStatus

def invoke(ctx: ScenarioContext) -> None:
    """
    Business rule: client.registration - create test client with specific status
    Command Pattern implementation for BDD step
    """
    client_id = ClientId(ctx.client_id)
    # GREEN Stage 1: Hardcoded fake implementation
    raise NotImplementedError("Client creation with status not implemented")
```

**File:** `tests/{feature}/when_submit_api_request.py`
```python
"""When I POST to endpoint - Screaming Architecture naming"""
from conftest import ScenarioContext
from brief_bridge.infrastructure.fastapi.main import app
import json

def invoke(ctx: ScenarioContext) -> None:
    """
    Execute API request through infrastructure layer
    Command Pattern implementation for BDD step
    """
    # GREEN Stage 1: Hardcoded fake implementation
    raise NotImplementedError("API request execution not implemented")
```

**File:** `tests/{feature}/then_verify_response_status.py`
```python
"""Then response status code verification - Screaming Architecture naming"""
from conftest import ScenarioContext

def invoke(ctx: ScenarioContext) -> None:
    """
    Verify API response follows expected contract
    Command Pattern implementation for BDD step
    """
    # GREEN Stage 1: Hardcoded fake implementation
    raise NotImplementedError("Response status verification not implemented")
```

### Step 5: Key Implementation Notes

**Pytest-BDD Critical Requirements:**
- Use `parsers.parse()` for parameter extraction
- Use `docstring` parameter name for multiline strings (FIXED NAME)  
- Use `datatable` parameter name for data tables (FIXED NAME)
- Use fixtures for test context instead of global variables
- All step functions should accept `context` fixture as first parameter

**Step Definition Patterns:**
```python
# Use absolute imports and domain-driven naming
from brief_bridge.domain.entities.client import Client
from brief_bridge.domain.value_objects.client_id import ClientId

# Simple parameter extraction with business meaning
@given(parsers.parse('client "{client_id}" exists'))
def given_client_exists_in_system(context, client_id):
    """Business rule: client.registration - setup test client"""
    raise NotImplementedError("Client entity creation not implemented")

# Multiline string handling with clear business intent
@when(parsers.parse('I POST to "{endpoint}" with:'))
def when_submit_api_request(context, endpoint, docstring):  # docstring = FIXED NAME
    """Execute API request through infrastructure layer"""
    raise NotImplementedError("API request execution not implemented")

# Data table handling with domain context
@given('following clients exist:')
def given_multiple_clients_exist_in_system(context, datatable):  # datatable = FIXED NAME
    """Business rule: test.data_setup - create multiple test clients"""
    raise NotImplementedError("Bulk client creation not implemented")
```

### Step 5: Validation

Test the skeleton structure:
```bash
# Should discover scenarios but not run them (all @skip)
python -m pytest tests/{feature}/ --collect-only

# Verify step collection works
python -m pytest tests/{feature}/ --collect-only -v

# Test a skipped scenario
python -m pytest tests/{feature}/ -v

# CRITICAL: Check for StepDefinitionNotFoundError
echo "🔍 Checking for framework errors..."
python -m pytest tests/ --collect-only 2>&1 | grep -E "(StepDefinitionNotFoundError|ImportError|ModuleNotFoundError)"
if [ $? -eq 0 ]; then
    echo "❌ Framework errors detected - fix before proceeding"
    exit 1
else
    echo "✅ No framework errors detected"
fi
```

Expected output:
- All scenarios discovered by pytest-bdd
- All scenarios marked as SKIPPED due to @skip marker
- No import errors from test file
- All step definitions correctly matched to feature file steps
- **CRITICAL**: No StepDefinitionNotFoundError or similar framework errors

## Implementation Guidelines

### Module Naming (Screaming Architecture)
Step module names should clearly describe business intent:

**Given Steps (Setup/Arrange):**
- `given_create_client_with_status.py` - Creates client with specific status
- `given_setup_test_environment.py` - Initializes test environment
- `given_register_multiple_clients.py` - Bulk client registration

**When Steps (Action/Act):**
- `when_submit_api_request.py` - Executes API endpoint calls
- `when_execute_command.py` - Runs business commands
- `when_send_heartbeat.py` - Sends client heartbeat

**Then Steps (Verification/Assert):**
- `then_verify_response_status.py` - Checks HTTP status codes
- `then_validate_response_body.py` - Validates response content
- `then_confirm_client_state.py` - Verifies client status

### Command Pattern Implementation
Each step module:
- Contains exactly ONE `invoke(ctx)` function
- Follows Command Pattern semantics
- Has business-descriptive docstring
- Uses absolute imports only
- Implements single responsibility

## Completion Checklist
- [ ] Feature directory created with step module structure
- [ ] Step wrapper file (`test_{feature}.py`) created with delegation only
- [ ] Root `conftest.py` contains `ScenarioContext` class and `context` fixture
- [ ] Individual step modules created with given_, when_, then_ prefixes
- [ ] Each step module has exactly ONE `invoke(ctx: ScenarioContext) -> None` function
- [ ] All step modules import `ScenarioContext` from conftest
- [ ] All step modules use Screaming Architecture naming
- [ ] Step wrapper uses `parsers.parse()` for parameter extraction
- [ ] Step wrappers handle `docstring` and `datatable` parameters correctly
- [ ] All step modules use absolute imports
- [ ] pytest can discover scenarios (but skips them due to @skip markers)
- [ ] No import errors from any test files
- [ ] All `invoke` functions raise NotImplementedError

## Verification Commands
```bash
# Should show discovered but skipped scenarios
python -m pytest tests/{feature}/ --collect-only

# Should show no errors
python -m pytest tests/{feature}/ --collect-only -v

# Should show all scenarios as skipped
python -m pytest tests/{feature}/ -v
```

## AI Assistant Behavior Guidelines

### After Each Major Step Completion
When you complete any major step in this phase (Step 1-6), you MUST:

1. **Commit your changes FIRST** using git with descriptive commit message
2. **Immediately report progress** (no user confirmation needed) using this exact format:

```
## Progress Report

**Current Phase:** Phase 2 - Walking Skeleton Creation
**What we completed:** [Describe what was just finished, e.g., "Created test infrastructure for submit_command feature with all step definition skeletons"]
**Next Phase:** [e.g., "Phase 3 - TDD Implementation" or "Continue Phase 2 - Step 5 Validation"]
**What we plan to do:** [Describe next concrete action, e.g., "Begin TDD cycle with first scenario activation and RED phase"]

**Files created/modified:** 
- tests/{feature}/test_{feature}.py                    # Step wrapper (delegates only)
- tests/{feature}/given_create_client_with_status.py
- tests/{feature}/when_submit_api_request.py
- tests/{feature}/then_verify_response_status.py
- [... other step modules as needed]

**Validation status:** [✅ Passed / ❌ Failed / ⏳ Pending]
```

3. **Automatically proceed to next step** after reporting (no user confirmation needed)

### Workflow Requirement
- **SEQUENCE CRITICAL**: Always commit FIRST, then report progress
- **AUTOMATIC FLOW**: After reporting, immediately continue to next step
- **NO CONFIRMATION NEEDED**: Do not wait for user approval to proceed
- **MEMORY BUILDING**: This structured reporting maintains context across interactions

## Next Step
After completing this phase, proceed to:
`03-tdd-red-green-refactor.md.prompt` - Begin TDD implementation cycle