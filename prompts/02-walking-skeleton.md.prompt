# Phase 2: Create Testing Walking Skeleton

## Objective
Build pytest-bdd infrastructure without real implementation - create the testing framework shell with NotImplementedError stubs.

## Prerequisites
- Feature file exists in `tests/{feature}/story.feature`
- All scenarios marked with @skip

## Required Reading
**MUST read these files before proceeding:**

Use the Read tool to load:
1. `tests/{specified_feature}/story.feature` - The feature file to implement
2. `docs/user-stories/{specified_feature}.md` - For step definitions reference
3. Current `pytest.ini` - To understand test configuration
4. `prompts/reference/python-coding-style.md.prompt` - For simplified architecture coding standards

## Step-by-Step Process

### Step 1: Analyze Feature File Steps
After reading the feature file, identify all unique step patterns:
- [ ] All Given steps (e.g., `Given client "{client_id}" exists with...`)
- [ ] All When steps (e.g., `When I POST to "{endpoint}" with:`)
- [ ] All Then steps (e.g., `Then response status code should be {code}`)

### Step 2: Create Test Infrastructure

Create this directory structure for `tests/{feature}/`:
```
tests/{feature}/
├── __init__.py                              # Empty file
├── story.feature                            # Already exists
├── test_{feature}.py                       # Step wrapper file (delegates to step modules)
├── given_create_client_with_status.py      # One step per module
├── given_setup_test_environment.py
├── when_submit_api_request.py              # One step per module
├── when_execute_command.py
├── then_verify_response_status.py          # One step per module
└── then_validate_response_body.py
```

### Step 3: Implement Step Wrapper File

**File:** `tests/{feature}/test_{feature}.py` (Only contains step wrappers)
```python
from pytest_bdd import scenarios, given, when, then, parsers
from tests.{feature} import given_create_client_with_status
from tests.{feature} import when_submit_api_request  
from tests.{feature} import then_verify_response_status

# Load all scenarios from feature file
scenarios('story.feature')

# Step wrappers - delegate to individual step modules
@given(parsers.parse('client "{client_id}" exists with status "{status}"'))
def given_client_with_status_exists(context, client_id, status):
    """Delegate to given_create_client_with_status step module"""
    context.client_id = client_id
    context.status = status
    given_create_client_with_status.invoke(context)

@when(parsers.parse('I POST to "{endpoint}" with:'))
def when_submit_request_to_api_endpoint(context, endpoint, docstring):
    """Delegate to when_submit_api_request step module"""
    context.endpoint = endpoint
    context.request_body = docstring
    when_submit_api_request.invoke(context)

@then(parsers.parse('response status code should be {status_code:d}'))
def then_api_response_has_expected_status(context, status_code):
    """Delegate to then_verify_response_status step module"""
    context.expected_status_code = status_code
    then_verify_response_status.invoke(context)
```

**Note:** The `context` fixture is now provided by root `conftest.py` with `TestContext` class.

### Step 4: Implement Individual Step Modules

Each step module contains exactly ONE `invoke` function following Command Pattern:

**File:** `tests/{feature}/given_create_client_with_status.py`
```python
"""Given client exists with status - Screaming Architecture naming"""
from conftest import TestContext
from brief_bridge.entities.client import Client
from brief_bridge.repositories.client_repository import ClientRepository

def invoke(ctx: TestContext) -> None:
    """
    Business rule: client.registration - create test client with specific status
    Command Pattern implementation for BDD step
    """
    # Simple data structures, dependency injection for repository
    client = Client(ctx.client_id, {"hostname": "test-host"})
    client.status = ctx.status
    # GREEN Stage 1: Hardcoded fake implementation  
    raise NotImplementedError("Client creation with status not implemented")
```

**File:** `tests/{feature}/when_submit_api_request.py`
```python
"""When I POST to endpoint - Screaming Architecture naming"""
from conftest import TestContext
from brief_bridge.use_cases.submit_command_use_case import SubmitCommandUseCase
import json

def invoke(ctx: TestContext) -> None:
    """
    Execute API request through UseCase layer
    Command Pattern implementation for BDD step
    """
    # Simple request structure - framework converts to dict
    request = {
        "client_id": ctx.client_id,
        "content": json.loads(ctx.request_body)["content"]
    }
    # GREEN Stage 1: Hardcoded fake implementation
    raise NotImplementedError("API request execution not implemented")
```

**File:** `tests/{feature}/then_verify_response_status.py`
```python
"""Then response status code verification - Screaming Architecture naming"""
from conftest import TestContext

def invoke(ctx: TestContext) -> None:
    """
    Verify API response follows expected contract
    Command Pattern implementation for BDD step
    """
    # GREEN Stage 1: Hardcoded fake implementation
    raise NotImplementedError("Response status verification not implemented")
```

### Step 5: Key Implementation Notes

**Pytest-BDD Critical Requirements:**
- Use `parsers.parse()` for parameter extraction
- Use `docstring` parameter name for multiline strings (FIXED NAME)  
- Use `datatable` parameter name for data tables (FIXED NAME)
- Use fixtures for test context instead of global variables
- All step functions should accept `context` fixture as first parameter

**Step Definition Patterns:**
```python
# Use absolute imports and domain-driven naming
from brief_bridge.domain.entities.client import Client
from brief_bridge.domain.value_objects.client_id import ClientId

# Simple parameter extraction with business meaning
@given(parsers.parse('client "{client_id}" exists'))
def given_client_exists_in_system(context, client_id):
    """Business rule: client.registration - setup test client"""
    raise NotImplementedError("Client entity creation not implemented")

# Multiline string handling with clear business intent
@when(parsers.parse('I POST to "{endpoint}" with:'))
def when_submit_api_request(context, endpoint, docstring):  # docstring = FIXED NAME
    """Execute API request through infrastructure layer"""
    raise NotImplementedError("API request execution not implemented")

# Data table handling with domain context
@given('following clients exist:')
def given_multiple_clients_exist_in_system(context, datatable):  # datatable = FIXED NAME
    """Business rule: test.data_setup - create multiple test clients"""
    raise NotImplementedError("Bulk client creation not implemented")
```

### Step 5: Validation

Test the skeleton structure:
```bash
# Should discover scenarios but not run them (all @skip)
python -m pytest tests/{feature}/ --collect-only

# Verify step collection works
python -m pytest tests/{feature}/ --collect-only -v

# Test a skipped scenario
python -m pytest tests/{feature}/ -v
```

Expected output:
- All scenarios discovered by pytest-bdd
- All scenarios marked as SKIPPED due to @skip marker
- No import errors from test file
- All step definitions correctly matched to feature file steps

## Implementation Guidelines

### Module Naming (Screaming Architecture)
Step module names should clearly describe business intent:

**Given Steps (Setup/Arrange):**
- `given_create_client_with_status.py` - Creates client with specific status
- `given_setup_test_environment.py` - Initializes test environment
- `given_register_multiple_clients.py` - Bulk client registration

**When Steps (Action/Act):**
- `when_submit_api_request.py` - Executes API endpoint calls
- `when_execute_command.py` - Runs business commands
- `when_send_heartbeat.py` - Sends client heartbeat

**Then Steps (Verification/Assert):**
- `then_verify_response_status.py` - Checks HTTP status codes
- `then_validate_response_body.py` - Validates response content
- `then_confirm_client_state.py` - Verifies client status

### Command Pattern Implementation
Each step module:
- Contains exactly ONE `invoke(ctx)` function
- Follows Command Pattern semantics
- Has business-descriptive docstring
- Uses absolute imports only
- Implements single responsibility

### Simplified Architecture for Walking Skeleton

**IMPORTANT: Framework ↔ UseCase ↔ Entity/Repository with Dependency Injection**

**✅ Do This (Simple & Testable):**
```python
# UseCase with dependency injection
class RegisterClientUseCase:
    def __init__(self, client_repository: ClientRepository):
        self._client_repository = client_repository  # Injected dependency
        
    async def execute(self, request: dict) -> dict:
        """request: {"client_id": "...", "info": {...}}"""
        client = Client(request["client_id"], request["info"])
        await self._client_repository.save(client)
        return {"client_id": client.client_id, "status": "registered"}

# Simple repository - concrete implementation
class ClientRepository:
    def __init__(self):
        self._clients = {}  # Simple dict storage
        
    async def save(self, client: Client) -> None:
        self._clients[client.client_id] = client
        
    async def find(self, client_id: str) -> Client:
        return self._clients.get(client_id)

# Simple entity with business logic
class Client:
    def __init__(self, client_id: str, info: dict):
        self.client_id = client_id
        self.info = info
        self.status = "ONLINE"
        self.availability = "IDLE"
        
    def can_accept_new_command(self) -> bool:
        """Business rule: client availability check"""
        return self.status == "ONLINE" and self.availability == "IDLE"
```

**❌ Avoid This (Over-Engineering):**
```python
# DON'T create complex abstractions in walking skeleton
class ClientRepositoryInterface(ABC): pass
class ClientFactory: pass
class ClientDomainService: pass
@dataclass class ClientId: pass  # Just use string
@dataclass class ClientInfo: pass  # Just use dict
```

**Walking Skeleton Architecture Rules:**
1. **Dependency Injection**: Inject repositories/services into UseCases
2. **Simple Data Structures**: Use dict/simple classes, avoid complex value objects
3. **Concrete Implementations**: Start with concrete classes, abstract later if needed
4. **Business Logic in Entities**: Keep domain rules in entity methods
5. **Two-Level Testing**: E2E via framework, Unit via UseCase with mocked dependencies

## Completion Checklist
- [ ] Feature directory created with step module structure
- [ ] Step wrapper file (`test_{feature}.py`) created with delegation only
- [ ] Root `conftest.py` contains `TestContext` class and `context` fixture
- [ ] Individual step modules created with given_, when_, then_ prefixes
- [ ] Each step module has exactly ONE `invoke(ctx: TestContext) -> None` function
- [ ] All step modules import `TestContext` from conftest
- [ ] All step modules use Screaming Architecture naming
- [ ] Step wrapper uses `parsers.parse()` for parameter extraction
- [ ] Step wrappers handle `docstring` and `datatable` parameters correctly
- [ ] All step modules use absolute imports
- [ ] pytest can discover scenarios (but skips them due to @skip markers)
- [ ] No import errors from any test files
- [ ] All `invoke` functions raise NotImplementedError

## Verification Commands
```bash
# Should show discovered but skipped scenarios
python -m pytest tests/{feature}/ --collect-only

# Should show no errors
python -m pytest tests/{feature}/ --collect-only -v

# Should show all scenarios as skipped
python -m pytest tests/{feature}/ -v
```

## AI Assistant Behavior Guidelines

### After Each Major Step Completion
When you complete any major step in this phase (Step 1-6), you MUST:

1. **Commit your changes FIRST** using git with descriptive commit message
2. **Immediately report progress** (no user confirmation needed) using this exact format:

```
## Progress Report

**Current Phase:** Phase 2 - Walking Skeleton Creation
**What we completed:** [Describe what was just finished, e.g., "Created test infrastructure for submit_command feature with all step definition skeletons"]
**Next Phase:** [e.g., "Phase 3 - TDD Implementation" or "Continue Phase 2 - Step 5 Validation"]
**What we plan to do:** [Describe next concrete action, e.g., "Begin TDD cycle with first scenario activation and RED phase"]

**Files created/modified:** 
- tests/{feature}/test_{feature}.py                    # Step wrapper (delegates only)
- tests/{feature}/given_create_client_with_status.py
- tests/{feature}/when_submit_api_request.py
- tests/{feature}/then_verify_response_status.py
- [... other step modules as needed]

**Validation status:** [✅ Passed / ❌ Failed / ⏳ Pending]
```

3. **Automatically proceed to next step** after reporting (no user confirmation needed)

### Workflow Requirement
- **SEQUENCE CRITICAL**: Always commit FIRST, then report progress
- **AUTOMATIC FLOW**: After reporting, immediately continue to next step
- **NO CONFIRMATION NEEDED**: Do not wait for user approval to proceed
- **MEMORY BUILDING**: This structured reporting maintains context across interactions

## Next Step
After completing this phase, proceed to:
`03-tdd-red-green-refactor.md.prompt` - Begin TDD implementation cycle