# Phase 2: Create Testing Walking Skeleton

## Objective
Build pytest-bdd infrastructure without real implementation - create the testing framework shell with NotImplementedError stubs.

## Prerequisites
- Feature file exists in `tests/{feature}/story.feature`
- All scenarios marked with @skip

## Required Reading
**MUST read these files before proceeding:**

Use the Read tool to load:
1. `tests/{specified_feature}/story.feature` - The feature file to implement
2. `docs/user-stories/{specified_feature}.md` - For step definitions reference
3. Current `pytest.ini` - To understand test configuration

## Step-by-Step Process

### Step 1: Analyze Feature File Steps
After reading the feature file, identify all unique step patterns:
- [ ] All Given steps (e.g., `Given client "{client_id}" exists with...`)
- [ ] All When steps (e.g., `When I POST to "{endpoint}" with:`)
- [ ] All Then steps (e.g., `Then response status code should be {code}`)

### Step 2: Create Test Infrastructure

Create this directory structure for `tests/{feature}/`:
```
tests/{feature}/
├── __init__.py                     # Empty file
├── story.feature                   # Already exists
└── test_{feature}.py              # Main test file with step definitions
```

### Step 3: Implement Main Test Orchestrator

**File:** `tests/{feature}/test_{feature}.py`
```python
import pytest
from pytest_bdd import scenarios, given, when, then, parsers

# Load all scenarios from feature file
scenarios('story.feature')

# Test context fixture
@pytest.fixture
def context():
    return type('Context', (), {
        'response': None,
        'test_data': {},
        'entities': []
    })()

# Step definitions with direct implementation
@given(parsers.parse('client "{client_id}" exists with status "{status}"'))
def given_client_exists(context, client_id, status):
    raise NotImplementedError("Step not implemented")

@when(parsers.parse('I POST to "{endpoint}" with:'))
def when_post_to_endpoint(context, endpoint, docstring):
    raise NotImplementedError("Step not implemented")

@then(parsers.parse('response status code should be {status_code:d}'))
def then_response_status_code(context, status_code):
    raise NotImplementedError("Step not implemented")
```

### Step 4: Key Implementation Notes

**Pytest-BDD Critical Requirements:**
- Use `parsers.parse()` for parameter extraction
- Use `docstring` parameter name for multiline strings (FIXED NAME)  
- Use `datatable` parameter name for data tables (FIXED NAME)
- Use fixtures for test context instead of global variables
- All step functions should accept `context` fixture as first parameter

**Step Definition Patterns:**
```python
# Simple parameter extraction
@given(parsers.parse('client "{client_id}" exists'))
def step_name(context, client_id):
    raise NotImplementedError("Step not implemented")

# Multiline string handling
@when(parsers.parse('I POST to "{endpoint}" with:'))
def step_name(context, endpoint, docstring):  # docstring = FIXED NAME
    raise NotImplementedError("Step not implemented")

# Data table handling  
@given('following clients exist:')
def step_name(context, datatable):  # datatable = FIXED NAME
    raise NotImplementedError("Step not implemented")
```

### Step 5: Validation

Test the skeleton structure:
```bash
# Should discover scenarios but not run them (all @skip)
python -m pytest tests/{feature}/ --collect-only

# Verify step collection works
python -m pytest tests/{feature}/ --collect-only -v

# Test a skipped scenario
python -m pytest tests/{feature}/ -v
```

Expected output:
- All scenarios discovered by pytest-bdd
- All scenarios marked as SKIPPED due to @skip marker
- No import errors from test file
- All step definitions correctly matched to feature file steps

## Implementation Guidelines

### Module Naming (Screaming Architecture)
Use descriptive module names that clearly indicate business intent:
- `Given client exists` → file could be named `create_client_setup.py`
- `When I POST to endpoint` → file could be named `execute_http_request.py`
- `Then response status code` → file could be named `verify_response_status.py`

**The key is: file names should clearly describe what they do**

## Completion Checklist
- [ ] Feature directory created with correct structure
- [ ] Main test file (`test_{feature}.py`) created with step definitions
- [ ] Context fixture created for test data sharing
- [ ] All step definitions use `parsers.parse()` for parameter extraction
- [ ] Step definitions handle `docstring` and `datatable` parameters correctly
- [ ] pytest can discover scenarios (but skips them due to @skip markers)
- [ ] No import errors from test file
- [ ] All step definitions raise NotImplementedError

## Verification Commands
```bash
# Should show discovered but skipped scenarios
python -m pytest tests/{feature}/ --collect-only

# Should show no errors
python -m pytest tests/{feature}/ --collect-only -v

# Should show all scenarios as skipped
python -m pytest tests/{feature}/ -v
```

## AI Assistant Behavior Guidelines

### After Each Major Step Completion
When you complete any major step in this phase (Step 1-6), you MUST:

1. **Commit your changes FIRST** using git with descriptive commit message
2. **Immediately report progress** (no user confirmation needed) using this exact format:

```
## Progress Report

**Current Phase:** Phase 2 - Walking Skeleton Creation
**What we completed:** [Describe what was just finished, e.g., "Created test infrastructure for submit_command feature with all step definition skeletons"]
**Next Phase:** [e.g., "Phase 3 - TDD Implementation" or "Continue Phase 2 - Step 5 Validation"]
**What we plan to do:** [Describe next concrete action, e.g., "Begin TDD cycle with first scenario activation and RED phase"]

**Files created/modified:** 
- tests/{feature}/test_{feature}.py
- tests/{feature}/given_steps.py
- tests/{feature}/when_steps.py 
- tests/{feature}/then_steps.py

**Validation status:** [✅ Passed / ❌ Failed / ⏳ Pending]
```

3. **Automatically proceed to next step** after reporting (no user confirmation needed)

### Workflow Requirement
- **SEQUENCE CRITICAL**: Always commit FIRST, then report progress
- **AUTOMATIC FLOW**: After reporting, immediately continue to next step
- **NO CONFIRMATION NEEDED**: Do not wait for user approval to proceed
- **MEMORY BUILDING**: This structured reporting maintains context across interactions

## Next Step
After completing this phase, proceed to:
`03-tdd-red-green-refactor.md.prompt` - Begin TDD implementation cycle