# Phase 3: TDD RED-GREEN-REFACTOR Cycle

## Architecture Selection
Use the same `ARCHITECTURE_TYPE` from Phase 2:

### üöÄ **SIMPLIFIED ARCHITECTURE**: `Framework ‚Üî UseCase ‚Üî Entity/Repository`
### üèõÔ∏è **CLEAN ARCHITECTURE**: `Domain ‚Üí Application ‚Üí Infrastructure` layers

---

## Objective
Implement production code using Test-Driven Development methodology with your chosen architecture, focusing on ONE scenario at a time.

## Prerequisites
- Feature file exists with @skip scenarios
- pytest-bdd walking skeleton completed
- All step definitions raise NotImplementedError

## Required Reading
**MUST read these files before proceeding:**

Use the Read tool to load:
1. `tests/{specified_feature}/story.feature` - Feature file with scenarios
2. `docs/domain-model.md` - For business rules and entities
3. `docs/clean-architecture-structure.md` - For code organization
4. `docs/user-stories/{specified_feature}.md` - For business rules reference
5. `prompts/reference/python-coding-style.md.prompt` - For coding standards

## TDD Process Overview

**Rule: Work on exactly ONE scenario at a time**
**Rule: Only run `pytest -m wip` during development**
**Rule: Only run `pytest` (full suite) after scenario completion**

## Step-by-Step TDD Cycle

### Step 1: Scenario Selection and Activation

1. **Select First Scenario**: Always start with the first @skip scenario in the feature file
2. **Change Marker**: Change `@skip` to `@wip` for the selected scenario
3. **Keep Others Skipped**: All other scenarios remain `@skip`

**Example:**
```gherkin
Feature: Submit Command
  
  @wip  # Changed from @skip - now active
  Scenario: Submit command to online idle client
    Given client "client-001" exists with status "ONLINE" and availability "IDLE"
    # ... rest of scenario
    
  @skip  # Still skipped
  Scenario: Submit command to offline client
    # ... scenario content
```

### Step 2: RED Phase (Failing Tests)

**Execute:**
```bash
pytest -m wip
```

**Expected Result:**
- Selected scenario should FAIL with NotImplementedError
- No other scenarios should run
- Focus on making ONLY this scenario pass

**Validation:**
- [ ] Only WIP scenario runs
- [ ] Test fails with NotImplementedError
- [ ] Ready to proceed to GREEN

### Step 3: GREEN Phase (Two-Stage Implementation)

**Execute throughout GREEN phases:**
```bash
pytest -m wip  # Only run during development
```

#### GREEN Stage 1: Production Invoke Chain Setup

**Objective:** Build complete production invoke chain with REAL classes and hardcoded return values.

**Key Principle:** 
- ‚úÖ **Production code**: Real classes, real methods, hardcoded returns
- ‚ùå **Tests**: NO fake production logic - only call production APIs

**Guidelines:**
- Build production components as required by test scenario
- Use the chosen `ARCHITECTURE_TYPE` from Phase 2 to determine import paths and patterns
- Follow your project's chosen architectural approach (simplified or clean)
- Every component has REAL classes with REAL methods
- Methods return hardcoded values (e.g. `return "hardcoded-uuid"`)
- Tests only contain test setup and call production code
- Goal: Complete production flow that makes test pass with minimal implementation

**Architecture-Specific Examples:**

### üöÄ Simplified Architecture - GREEN Stage 1
```python
# brief_bridge/entities/client.py - REAL class with hardcoded returns
class Client:
    def __init__(self, client_id: str, client_info: dict):
        self.client_id = client_id
        self.client_info = client_info
        
    def get_status(self) -> str:
        return "ONLINE"  # Hardcoded for Stage 1

# brief_bridge/use_cases/register_client.py - REAL use case
class RegisterClientUseCase:
    def __init__(self, client_repository):
        self.client_repository = client_repository
        
    async def execute(self, request) -> dict:
        return {"client_id": "hardcoded-uuid-123", "status": "ONLINE"}

# Test step module: Only calls production code - NO fake logic
def invoke(ctx: ScenarioContext) -> None:
    from brief_bridge.use_cases.register_client import RegisterClientUseCase
    use_case = RegisterClientUseCase(ctx.client_repository)
    response = asyncio.run(use_case.execute(ctx.request))
    ctx.created_client_id = response["client_id"]
```

### üèõÔ∏è Clean Architecture - GREEN Stage 1  
```python
# brief_bridge/domain/entities/client.py - REAL entity with hardcoded returns
class Client:
    def __init__(self, client_id: ClientId, client_info: ClientInfo):
        self._client_id = client_id
        self._client_info = client_info
        
    def get_status(self) -> ClientStatus:
        return ClientStatus.ONLINE  # Hardcoded for Stage 1

# brief_bridge/application/use_cases/register_client.py - REAL use case
class RegisterClientUseCase:
    def __init__(self, client_repo: ClientRepository):
        self._client_repo = client_repo
        
    async def execute(self, request: RegisterClientRequest) -> RegisterClientResponse:
        return RegisterClientResponse(client_id="hardcoded-uuid-123", status=ClientStatus.ONLINE)

# Test step module: Only calls production code - NO fake logic
def invoke(ctx: ScenarioContext) -> None:
    from brief_bridge.application.use_cases.register_client import RegisterClientUseCase
    use_case = RegisterClientUseCase(ctx.client_repository)
    response = asyncio.run(use_case.execute(ctx.request))
    ctx.created_client_id = response.client_id
```

**Stage 1 Validation:**
```bash
pytest -m wip  # Should PASS
```
- [ ] Test passes with REAL production classes (hardcoded returns)
- [ ] NO fake logic in test code - tests only call production
- [ ] Ready for Stage 2 business rule implementation

#### GREEN Stage 2: Business Rule Implementation

**Objective:** Replace fake implementations with real business logic for current scenario.

**Guidelines:**
- Reference `docs/domain-model.md` for business rules
- **Only implement business rules tested by current scenario**
- Replace hardcoded logic with proper domain logic
- Keep CA structure from Stage 1

**Business Rule Implementation:**
1. Identify which business rules current scenario tests
2. Implement those rules in appropriate domain entities/services
3. Update step definitions to use real domain logic

**Architecture-Specific Examples:**

### üöÄ Simplified Architecture - GREEN Stage 2
```python
# brief_bridge/entities/client.py (enhanced with real business logic)
class Client:
    def __init__(self, client_id: str, client_info: dict):
        self.client_id = client_id
        self.client_info = client_info
        self.status = "ONLINE"  # Business rule: client.registration
        self.availability = "IDLE"
        self.last_heartbeat = datetime.now(timezone.utc)
    
    def mark_online(self):
        # Business rule: client.heartbeat
        self.status = "ONLINE"
        self.last_heartbeat = datetime.now(timezone.utc)
    
    def mark_offline(self):
        self.status = "OFFLINE"
        self.availability = "IDLE"

# tests/{feature}/given_create_client_with_status.py (updated)
def invoke(ctx: ScenarioContext) -> None:
    from brief_bridge.entities.client import Client
    
    # Now with proper business rule implementation
    client_info = {"os": "Linux", "architecture": "x86_64", "version": "Ubuntu 20.04"}
    client = Client(ctx.client_id, client_info)
    
    # Business rule: client status management
    if ctx.status == "ONLINE":
        client.mark_online()
    elif ctx.status == "OFFLINE":
        client.mark_offline()
    
    asyncio.run(ctx.client_repository.save(client))
```

### üèõÔ∏è Clean Architecture - GREEN Stage 2
```python
# brief_bridge/domain/entities/client.py (enhanced with real business logic)
class Client:
    def __init__(self, client_id: ClientId, client_info: ClientInfo):
        self._client_id = client_id
        self._client_info = client_info
        self._status = ClientStatus.ONLINE  # Business rule: client.registration
        self._availability = ClientAvailability.IDLE
        self._last_heartbeat = datetime.now(timezone.utc)
    
    def mark_online(self):
        # Business rule: client.heartbeat
        self._status = ClientStatus.ONLINE
        self._last_heartbeat = datetime.now(timezone.utc)
    
    def mark_offline(self):
        self._status = ClientStatus.OFFLINE
        self._availability = ClientAvailability.IDLE

# tests/{feature}/given_create_client_with_status.py (updated)
def invoke(ctx: ScenarioContext) -> None:
    from brief_bridge.domain.entities.client import Client
    from brief_bridge.domain.value_objects.client_id import ClientId
    from brief_bridge.domain.value_objects.client_info import ClientInfo
    
    # Now with proper business rule implementation
    client_info = ClientInfo(os="Linux", architecture="x86_64", version="Ubuntu 20.04")
    client = Client(ClientId(ctx.client_id), client_info)
    
    # Business rule: client status management
    if ctx.status == "ONLINE":
        client.mark_online()
    elif ctx.status == "OFFLINE":
        client.mark_offline()
    
    asyncio.run(ctx.client_repository.save(client))
```

**Stage 2 Validation:**
```bash
pytest -m wip  # Should still PASS
```
- [ ] Test still passes
- [ ] Real business rules implemented
- [ ] Only current scenario's business rules addressed

### Step 4: REFACTOR Phase (Code Quality)

**Objective:** Improve code quality without changing functionality.

**Guidelines:**
- CA structure exists from GREEN Stage 1
- Business rules implemented from GREEN Stage 2
- **Focus ONLY on code quality improvements**
- Do not add new features or change functionality

**Continue running:**
```bash
pytest -m wip  # Must pass throughout refactoring
```

**Refactor Focus Areas:**
- Extract methods for better readability
- Remove duplicate code
- Improve variable naming
- Simplify conditional logic
- Add helpful comments

**Example Refactoring:**
```python
# Before refactoring
@given('client "{client_id}" exists with status "{status}" and availability "{availability}"')
def given_client_exists(client_id, status, availability, client_repository):
    client_info = ClientInfo(os="Linux", architecture="x86_64", version="Ubuntu 20.04")
    client = Client(ClientId(client_id), client_info)
    if status == "ONLINE":
        client.mark_online()
    elif status == "OFFLINE":
        client.mark_offline()
    asyncio.run(client_repository.save(client))

# After refactoring (improved readability)
@given('client "{client_id}" exists with status "{status}" and availability "{availability}"')
def given_client_exists(client_id, status, availability, client_repository):
    client_info = ClientInfo.create_default_linux()  # Extracted method
    client = Client(ClientId(client_id), client_info)
    client.set_status(ClientStatus(status))  # Simplified status handling
    asyncio.run(client_repository.save(client))
```

**REFACTOR Validation:**
```bash
pytest -m wip  # Must still pass
```
- [ ] Test still passes
- [ ] Code quality improved
- [ ] No functionality changes
- [ ] Same test behavior maintained

### Step 5: Scenario Completion

**Final Steps:**
1. **Remove WIP Mark**: Change `@wip` back to no marker
2. **Run Full Test Suite**: Execute `pytest` (without markers)
3. **Validate**: All tests must pass (current + previously completed)

**Example Completed Scenario:**
```gherkin
Feature: Submit Command
  
  # No marker - completed and ready for full test runs
  Scenario: Submit command to online idle client
    Given client "client-001" exists with status "ONLINE" and availability "IDLE"
    # ... rest of scenario
    
  @skip  # Next scenario to implement
  Scenario: Submit command to offline client
    # ... scenario content
```

**Full Validation:**
```bash
pytest  # Run complete test suite
```
- [ ] Current scenario passes
- [ ] All previously completed scenarios still pass
- [ ] No regressions introduced

## Phase Completion Checklist

### Overall Validation
- [ ] GREEN Stage 1: CA structure with fake implementations ‚úÖ
- [ ] GREEN Stage 2: Business rules for current scenario ‚úÖ  
- [ ] REFACTOR: Code quality improvements ‚úÖ
- [ ] WIP marker removed from completed scenario
- [ ] Full test suite passes without regressions

## Critical Rules

### DO
- ‚úÖ Work on exactly one scenario at a time
- ‚úÖ Use `pytest -m wip` during TDD cycle
- ‚úÖ Run `pytest` (full suite) only after completion
- ‚úÖ Follow GREEN Stage 1 ‚Üí Stage 2 ‚Üí REFACTOR sequence
- ‚úÖ Remove @wip marker when scenario complete

### DON'T  
- ‚ùå Work on multiple scenarios simultaneously
- ‚ùå Skip GREEN stages or REFACTOR
- ‚ùå Run full test suite during TDD cycle
- ‚ùå Implement business rules before GREEN Stage 2
- ‚ùå Add features not required by current scenario
- ‚ùå Add business endpoints directly to main.py (use routers instead)
- ‚ùå **CRITICAL**: Put fake production logic in test code (violates responsibility separation)
- ‚ùå **CRITICAL**: Put test-specific methods in production classes

## AI Assistant Behavior Guidelines

### After Each TDD Phase Completion
When you complete any TDD phase (RED, GREEN Stage 1, GREEN Stage 2, REFACTOR), you MUST:

1. **Commit your changes FIRST** using git with descriptive commit message
2. **Immediately report progress** (no user confirmation needed) using this exact format:

```
## Progress Report

**Current Phase:** Phase 3 - TDD Implementation
**TDD Stage:** [RED / GREEN Stage 1 / GREEN Stage 2 / REFACTOR]
**Scenario:** [e.g., "Submit command to online idle client"]
**What we completed:** [Describe what was just finished, e.g., "GREEN Stage 1 - Created CA infrastructure shell with fake implementations"]
**Next Phase:** [e.g., "GREEN Stage 2" or "REFACTOR" or "Scenario Completion"]
**What we plan to do:** [Describe next concrete action, e.g., "Implement real business rules for client.registration and command.target_validation"]

**Test Status:** [‚úÖ pytest -m wip PASSING / ‚ùå FAILING]
**Files modified:** 
- brief_bridge/domain/entities/...
- brief_bridge/infrastructure/...
- tests/{feature}/given_create_client_with_status.py

**Business Rules Implemented:** [List specific business rules addressed]
```

3. **Always run the appropriate test command** and report results:
   - During TDD: `pytest -m wip`
   - After completion: `pytest` (full suite)
4. **Automatically proceed to next TDD phase** (no user confirmation needed)

### After Scenario Completion
When you complete an entire scenario, you MUST:

1. **Final commit FIRST** with scenario completion message
2. **Immediately provide full validation report** (no user confirmation needed):

```
## Scenario Completion Report

**Completed Scenario:** [Scenario name]
**Feature:** [Feature name]
**Phase Status:** ‚úÖ Scenario Complete - Ready for next iteration
**What we completed:** [Full scenario implementation summary]
**Next Phase:** Phase 4 - Iteration Cycle
**What we plan to do:** [Next scenario selection or feature completion]

**Validation Results:**
- ‚úÖ pytest -m wip removed (no WIP scenarios)
- ‚úÖ pytest (full suite passing)
- ‚úÖ All TDD phases completed
- ‚úÖ Business rules properly implemented

**Architecture Status:**
- ‚úÖ Clean Architecture maintained
- ‚úÖ Domain logic in correct layer
- ‚úÖ Test responsibilities properly separated
```

3. **Automatically proceed to next phase** after reporting (no user confirmation needed)

### Workflow Requirements
- **SEQUENCE CRITICAL**: Always commit FIRST, then report progress  
- **AUTOMATIC TDD FLOW**: After each phase report, immediately continue to next TDD phase
- **SCENARIO COMPLETION**: After scenario completion report, automatically proceed to iteration cycle
- **NO CONFIRMATION NEEDED**: Never wait for user approval during TDD cycle
- **CONTINUOUS MOMENTUM**: Maintain development flow with structured progress tracking

## Next Step
After completing this scenario, proceed to:
`04-iteration-cycle.md.prompt` - Move to next scenario or feature