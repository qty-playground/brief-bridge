# Phase 3: TDD RED-GREEN-REFACTOR Cycle

## Objective
Implement production code using Test-Driven Development methodology with Clean Architecture, focusing on ONE scenario at a time.

## Prerequisites
- Feature file exists with @skip scenarios
- pytest-bdd walking skeleton completed
- All step definitions raise NotImplementedError

## Required Reading
**MUST read these files before proceeding:**

Use the Read tool to load:
1. `tests/{specified_feature}/story.feature` - Feature file with scenarios
2. `docs/domain-model.md` - For business rules and entities
3. `docs/clean-architecture-structure.md` - For code organization
4. `docs/user-stories/{specified_feature}.md` - For business rules reference
5. `prompts/reference/python-coding-style.md.prompt` - For coding standards

## TDD Process Overview

**Rule: Work on exactly ONE scenario at a time**
**Rule: Only run `pytest -m wip` during development**
**Rule: Only run `pytest` (full suite) after scenario completion**

## Step-by-Step TDD Cycle

### Step 1: Scenario Selection and Activation

1. **Select First Scenario**: Always start with the first @skip scenario in the feature file
2. **Change Marker**: Change `@skip` to `@wip` for the selected scenario
3. **Keep Others Skipped**: All other scenarios remain `@skip`

**Example:**
```gherkin
Feature: Submit Command
  
  @wip  # Changed from @skip - now active
  Scenario: Submit command to online idle client
    Given client "client-001" exists with status "ONLINE" and availability "IDLE"
    # ... rest of scenario
    
  @skip  # Still skipped
  Scenario: Submit command to offline client
    # ... scenario content
```

### Step 2: RED Phase (Failing Tests)

**Execute:**
```bash
pytest -m wip
```

**Expected Result:**
- Selected scenario should FAIL with NotImplementedError
- No other scenarios should run
- Focus on making ONLY this scenario pass

**Validation:**
- [ ] Only WIP scenario runs
- [ ] Test fails with NotImplementedError
- [ ] Ready to proceed to GREEN

### Step 3: GREEN Phase (Two-Stage Implementation)

**Execute throughout GREEN phases:**
```bash
pytest -m wip  # Only run during development
```

#### GREEN Stage 1: Infrastructure Shell

**Objective:** Create minimal Clean Architecture structure with fake/hardcoded implementations.

**Guidelines:**
- Build CA skeleton components ONLY
- Use hardcoded/fake data to make test pass
- **Completely ignore business rules**
- **Completely ignore proper implementation**
- Goal: Test goes from RED to GREEN with minimal structure

**Required CA Components:**
Based on `docs/clean-architecture-structure.md`, create:
- Domain entities (with create_fake methods)
- Domain value objects (basic implementations)
- Repository interfaces (in domain layer)
- Repository implementations (in infrastructure layer)
- FastAPI routers (NOT endpoints in main.py)
- Basic step definition implementations

**Example Implementation (following python-coding-style.md.prompt):**
```python
# brief_bridge/domain/entities/client.py
class Client:
    def __init__(self, client_id: ClientId, client_info: ClientInfo):
        self._client_id = client_id
        self._client_info = client_info
    
    @classmethod
    def create_fake_for_testing(cls, client_id_str: str, status: str, availability: str) -> Client:
        fake_client_id = ClientId(client_id_str)
        fake_client_info = ClientInfo.create_default_linux()
        client = cls(fake_client_id, fake_client_info)
        client._fake_status = status
        return client

# brief_bridge/infrastructure/fastapi/routers/clients.py
from fastapi import APIRouter
from brief_bridge.application.dto.client_dto import RegisterClientRequest

router = APIRouter()

@router.post("/register")
async def register_client(request: RegisterClientRequest) -> dict:
    return {"client_id": "fake-uuid", "status": "ONLINE"}

# brief_bridge/main.py - ONLY include router, no business endpoints
from brief_bridge.infrastructure.fastapi.routers import clients
app.include_router(clients.router, prefix="/clients", tags=["clients"])

# tests/{feature}/step_definitions.py
@given(parsers.parse('client "{client_id}" exists with status "{status}"'))
def given_client_exists_with_status(context: dict, client_id: str, status: str, client_repository: ClientRepository) -> None:
    fake_client = Client.create_fake_for_testing(client_id, status, "IDLE")
    asyncio.run(client_repository.save(fake_client))
```

**Stage 1 Validation:**
```bash
pytest -m wip  # Should PASS
```
- [ ] Test passes with hardcoded/fake implementations
- [ ] CA structure created (entities, repositories, etc.)
- [ ] No business rules implemented yet

#### GREEN Stage 2: Business Rule Implementation

**Objective:** Replace fake implementations with real business logic for current scenario.

**Guidelines:**
- Reference `docs/domain-model.md` for business rules
- **Only implement business rules tested by current scenario**
- Replace hardcoded logic with proper domain logic
- Keep CA structure from Stage 1

**Business Rule Implementation:**
1. Identify which business rules current scenario tests
2. Implement those rules in appropriate domain entities/services
3. Update step definitions to use real domain logic

**Example Implementation:**
```python
# brief_bridge/domain/entities/client.py (enhanced)
class Client:
    def __init__(self, client_id: ClientId, client_info: ClientInfo):
        self._client_id = client_id
        self._client_info = client_info
        self._status = ClientStatus.ONLINE  # Business rule: client.registration
        self._availability = ClientAvailability.IDLE
        self._last_heartbeat = datetime.now(timezone.utc)
    
    def mark_online(self):
        # Business rule: client.heartbeat
        self._status = ClientStatus.ONLINE
        self._last_heartbeat = datetime.now(timezone.utc)
    
    def mark_offline(self):
        self._status = ClientStatus.OFFLINE
        self._availability = ClientAvailability.IDLE

# tests/{feature}/given_steps.py (updated)
@given('client "{client_id}" exists with status "{status}" and availability "{availability}"')
def given_client_exists(client_id, status, availability, client_repository):
    # Now with proper business rule implementation
    client_info = ClientInfo(os="Linux", architecture="x86_64", version="Ubuntu 20.04")
    client = Client(ClientId(client_id), client_info)
    
    # Business rule: client status management
    if status == "ONLINE":
        client.mark_online()
    elif status == "OFFLINE":
        client.mark_offline()
    
    asyncio.run(client_repository.save(client))
```

**Stage 2 Validation:**
```bash
pytest -m wip  # Should still PASS
```
- [ ] Test still passes
- [ ] Real business rules implemented
- [ ] Only current scenario's business rules addressed

### Step 4: REFACTOR Phase (Code Quality)

**Objective:** Improve code quality without changing functionality.

**Guidelines:**
- CA structure exists from GREEN Stage 1
- Business rules implemented from GREEN Stage 2
- **Focus ONLY on code quality improvements**
- Do not add new features or change functionality

**Continue running:**
```bash
pytest -m wip  # Must pass throughout refactoring
```

**Refactor Focus Areas:**
- Extract methods for better readability
- Remove duplicate code
- Improve variable naming
- Simplify conditional logic
- Add helpful comments

**Example Refactoring:**
```python
# Before refactoring
@given('client "{client_id}" exists with status "{status}" and availability "{availability}"')
def given_client_exists(client_id, status, availability, client_repository):
    client_info = ClientInfo(os="Linux", architecture="x86_64", version="Ubuntu 20.04")
    client = Client(ClientId(client_id), client_info)
    if status == "ONLINE":
        client.mark_online()
    elif status == "OFFLINE":
        client.mark_offline()
    asyncio.run(client_repository.save(client))

# After refactoring (improved readability)
@given('client "{client_id}" exists with status "{status}" and availability "{availability}"')
def given_client_exists(client_id, status, availability, client_repository):
    client_info = ClientInfo.create_default_linux()  # Extracted method
    client = Client(ClientId(client_id), client_info)
    client.set_status(ClientStatus(status))  # Simplified status handling
    asyncio.run(client_repository.save(client))
```

**REFACTOR Validation:**
```bash
pytest -m wip  # Must still pass
```
- [ ] Test still passes
- [ ] Code quality improved
- [ ] No functionality changes
- [ ] Same test behavior maintained

### Step 5: Scenario Completion

**Final Steps:**
1. **Remove WIP Mark**: Change `@wip` back to no marker
2. **Run Full Test Suite**: Execute `pytest` (without markers)
3. **Validate**: All tests must pass (current + previously completed)

**Example Completed Scenario:**
```gherkin
Feature: Submit Command
  
  # No marker - completed and ready for full test runs
  Scenario: Submit command to online idle client
    Given client "client-001" exists with status "ONLINE" and availability "IDLE"
    # ... rest of scenario
    
  @skip  # Next scenario to implement
  Scenario: Submit command to offline client
    # ... scenario content
```

**Full Validation:**
```bash
pytest  # Run complete test suite
```
- [ ] Current scenario passes
- [ ] All previously completed scenarios still pass
- [ ] No regressions introduced

## Phase Completion Checklist

### Overall Validation
- [ ] GREEN Stage 1: CA structure with fake implementations ✅
- [ ] GREEN Stage 2: Business rules for current scenario ✅  
- [ ] REFACTOR: Code quality improvements ✅
- [ ] WIP marker removed from completed scenario
- [ ] Full test suite passes without regressions

## Critical Rules

### DO
- ✅ Work on exactly one scenario at a time
- ✅ Use `pytest -m wip` during TDD cycle
- ✅ Run `pytest` (full suite) only after completion
- ✅ Follow GREEN Stage 1 → Stage 2 → REFACTOR sequence
- ✅ Remove @wip marker when scenario complete

### DON'T  
- ❌ Work on multiple scenarios simultaneously
- ❌ Skip GREEN stages or REFACTOR
- ❌ Run full test suite during TDD cycle
- ❌ Implement business rules before GREEN Stage 2
- ❌ Add features not required by current scenario
- ❌ Add business endpoints directly to main.py (use routers instead)

## AI Assistant Behavior Guidelines

### After Each TDD Phase Completion
When you complete any TDD phase (RED, GREEN Stage 1, GREEN Stage 2, REFACTOR), you MUST:

1. **Commit your changes FIRST** using git with descriptive commit message
2. **Immediately report progress** (no user confirmation needed) using this exact format:

```
## Progress Report

**Current Phase:** Phase 3 - TDD Implementation
**TDD Stage:** [RED / GREEN Stage 1 / GREEN Stage 2 / REFACTOR]
**Scenario:** [e.g., "Submit command to online idle client"]
**What we completed:** [Describe what was just finished, e.g., "GREEN Stage 1 - Created CA infrastructure shell with fake implementations"]
**Next Phase:** [e.g., "GREEN Stage 2" or "REFACTOR" or "Scenario Completion"]
**What we plan to do:** [Describe next concrete action, e.g., "Implement real business rules for client.registration and command.target_validation"]

**Test Status:** [✅ pytest -m wip PASSING / ❌ FAILING]
**Files modified:** 
- brief_bridge/domain/entities/...
- brief_bridge/infrastructure/...
- tests/{feature}/given_steps.py

**Business Rules Implemented:** [List specific business rules addressed]
```

3. **Always run the appropriate test command** and report results:
   - During TDD: `pytest -m wip`
   - After completion: `pytest` (full suite)
4. **Automatically proceed to next TDD phase** (no user confirmation needed)

### After Scenario Completion
When you complete an entire scenario, you MUST:

1. **Final commit FIRST** with scenario completion message
2. **Immediately provide full validation report** (no user confirmation needed):

```
## Scenario Completion Report

**Completed Scenario:** [Scenario name]
**Feature:** [Feature name]
**Phase Status:** ✅ Scenario Complete - Ready for next iteration
**What we completed:** [Full scenario implementation summary]
**Next Phase:** Phase 4 - Iteration Cycle
**What we plan to do:** [Next scenario selection or feature completion]

**Validation Results:**
- ✅ pytest -m wip removed (no WIP scenarios)
- ✅ pytest (full suite passing)
- ✅ All TDD phases completed
- ✅ Business rules properly implemented

**Architecture Status:**
- ✅ Clean Architecture maintained
- ✅ Domain logic in correct layer
- ✅ Test responsibilities properly separated
```

3. **Automatically proceed to next phase** after reporting (no user confirmation needed)

### Workflow Requirements
- **SEQUENCE CRITICAL**: Always commit FIRST, then report progress  
- **AUTOMATIC TDD FLOW**: After each phase report, immediately continue to next TDD phase
- **SCENARIO COMPLETION**: After scenario completion report, automatically proceed to iteration cycle
- **NO CONFIRMATION NEEDED**: Never wait for user approval during TDD cycle
- **CONTINUOUS MOMENTUM**: Maintain development flow with structured progress tracking

## Next Step
After completing this scenario, proceed to:
`04-iteration-cycle.md.prompt` - Move to next scenario or feature